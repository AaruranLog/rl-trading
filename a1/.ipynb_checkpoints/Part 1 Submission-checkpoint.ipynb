{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    '''A simple MDP class.  It includes the following members'''\n",
    "\n",
    "    def __init__(self,T,R,discount):\n",
    "        '''Constructor for the MDP class\n",
    "\n",
    "        Inputs:\n",
    "        T -- Transition function: |A| x |S| x |S'| array\n",
    "        R -- Reward function: |A| x |S| array\n",
    "        discount -- discount factor: scalar in [0,1)\n",
    "\n",
    "        The constructor verifies that the inputs are valid and sets\n",
    "        corresponding variables in a MDP object'''\n",
    "\n",
    "        assert T.ndim == 3, \"Invalid transition function: it should have 3 dimensions\"\n",
    "        self.nActions = T.shape[0]\n",
    "        self.nStates = T.shape[1]\n",
    "        assert T.shape == (self.nActions,self.nStates,self.nStates), \"Invalid transition function: it has dimensionality \" + repr(T.shape) + \", but it should be (nActions,nStates,nStates)\"\n",
    "        assert (abs(T.sum(2)-1) < 1e-5).all(), \"Invalid transition function: some transition probability does not equal 1\"\n",
    "        self.T = T\n",
    "        assert R.ndim == 2, \"Invalid reward function: it should have 2 dimensions\" \n",
    "        assert R.shape == (self.nActions,self.nStates), \"Invalid reward function: it has dimensionality \" + repr(R.shape) + \", but it should be (nActions,nStates)\"\n",
    "        self.R = R\n",
    "        assert 0 <= discount < 1, \"Invalid discount factor: it should be in [0,1)\"\n",
    "        self.discount = discount\n",
    "        \n",
    "    def valueIteration(self,initialV,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Value iteration procedure\n",
    "        V <-- max_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "        \n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        # V = np.zeros(self.nStates)\n",
    "        print('Value iteration...')\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = 0\n",
    "        while iterId < nIterations:\n",
    "            new_V = (self.R + self.discount * self.T @ V).max(axis=0)\n",
    "            assert new_V.shape == V.shape, f\"{new_V.shape} should be {V.shape}\"\n",
    "            epsilon = np.linalg.norm(new_V - V, np.inf)\n",
    "            V = new_V\n",
    "            iterId += 1\n",
    "            if epsilon < tolerance:\n",
    "                break\n",
    "#             if iterId % 100 == 0:\n",
    "#                 print(f'iteration {iterId},\\t epsilon = {epsilon}')\n",
    "            \n",
    "        print(f'[V,iterId,epsilon] = {[V,iterId,epsilon]}')\n",
    "        return [V,iterId,epsilon]\n",
    "\n",
    "    def extractPolicy(self,V):\n",
    "        '''Procedure to extract a policy from a value function\n",
    "        pi <-- argmax_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        V -- Value function: array of |S| entries\n",
    "\n",
    "        Output:\n",
    "        policy -- Policy: array of |S| entries'''\n",
    "\n",
    "        policy = np.argmax(self.R + self.discount * (self.T @ V), axis=0)\n",
    "        # assert policy.shape == V.shape\n",
    "        return policy\n",
    "\n",
    "    def evaluatePolicy(self,policy):\n",
    "        '''Evaluate a policy by solving a system of linear equations\n",
    "        V^pi = R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Input:\n",
    "        policy -- Policy: array of |S| entries\n",
    "\n",
    "        Ouput:\n",
    "        V -- Value function: array of |S| entries'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        # V = np.zeros(self.nStates)\n",
    "        # print(f'policy: {policy.shape}, {policy}')\n",
    "\n",
    "        pi_state = np.arange(self.nStates)\n",
    "        R_pi = self.R[policy, pi_state]\n",
    "        # print(f'R_pi: {R_pi.shape}, R: {self.R.shape}')\n",
    "\n",
    "        T_pi = self.T[policy, pi_state]\n",
    "        # print(f'T_pi: {T_pi.shape}, T: {self.T.shape}')\n",
    "\n",
    "        A = np.eye(self.nStates) - self.discount * T_pi\n",
    "        # print(f'A: {A.shape}')\n",
    "\n",
    "        V = np.linalg.solve(A, R_pi)\n",
    "        # assert V.shape == policy.shape, f\"got {V.shape} but expected {policy.shape}\"\n",
    "        return V\n",
    "        \n",
    "    def policyIteration(self,initialPolicy,nIterations=np.inf):\n",
    "        '''Policy iteration procedure: alternate between policy\n",
    "        evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and policy\n",
    "        improvement (pi <-- argmax_a R^a + gamma T^a V^pi).\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        nIterations -- limit on # of iterations: scalar (default: inf)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        # policy = np.zeros(self.nStates)\n",
    "        policy = initialPolicy\n",
    "        V = np.zeros(self.nStates)\n",
    "        iterId = 0\n",
    "        while iterId < nIterations:\n",
    "            V = self.evaluatePolicy(policy)\n",
    "            new_policy = self.extractPolicy(V)\n",
    "            if (new_policy == policy).all():\n",
    "                break\n",
    "            assert new_policy.shape == policy.shape\n",
    "            policy = new_policy\n",
    "            iterId += 1\n",
    "\n",
    "        return [policy,V,iterId]\n",
    "            \n",
    "    def evaluatePolicyPartially(self,policy,initialV,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Partial policy evaluation:\n",
    "        Repeat V^pi <-- R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Inputs:\n",
    "        policy -- Policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        policy = policy.astype(int)\n",
    "\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "        pi_state = np.arange(self.nStates)\n",
    "\n",
    "        R_pi = self.R[policy, pi_state]\n",
    "        # print(f'R_pi: {R_pi.shape}, R: {self.R.shape}')\n",
    "\n",
    "        T_pi = self.T[policy, pi_state]\n",
    "        # print(f'T_pi: {T_pi.shape}, T: {self.T.shape}')\n",
    "        while iterId < nIterations and epsilon > tolerance:\n",
    "            new_V = R_pi + self.discount * T_pi @ V\n",
    "            delta_V = new_V - V\n",
    "            epsilon = np.linalg.norm(delta_V, np.inf)\n",
    "            # print(f'epsilon = {epsilon}')\n",
    "\n",
    "            V = new_V\n",
    "            iterId += 1\n",
    "\n",
    "        return [V,iterId,epsilon]\n",
    "\n",
    "    def modifiedPolicyIteration(self,initialPolicy,initialV,nEvalIterations=5,nIterations=np.inf,tolerance=0.01):\n",
    "        '''Modified policy iteration procedure: alternate between\n",
    "        partial policy evaluation (repeat a few times V^pi <-- R^pi + gamma T^pi V^pi)\n",
    "        and policy improvement (pi <-- argmax_a R^a + gamma T^a V^pi)\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nEvalIterations -- limit on # of iterations to be performed in each partial policy evaluation: scalar (default: 5)\n",
    "        nIterations -- limit on # of iterations to be performed in modified policy iteration: scalar (default: inf)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = np.zeros(self.nStates)\n",
    "        V = np.zeros(self.nStates)\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "\n",
    "        while iterId < nIterations and epsilon > tolerance:\n",
    "            V, _, _ = self.evaluatePolicyPartially(policy, V,  nIterations=nEvalIterations)\n",
    "            policy = self.extractPolicy(V)\n",
    "            new_V = (self.R + self.discount * self.T @ V).max(axis=0)\n",
    "            epsilon = np.linalg.norm(new_V - V, np.inf)\n",
    "            iterId += 1\n",
    "\n",
    "        return [policy,V,iterId,epsilon]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Construct a simple maze MDP\n",
    "\n",
    "  Grid world layout:\n",
    "\n",
    "  ---------------------\n",
    "  |  0 |  1 |  2 |  3 |\n",
    "  ---------------------\n",
    "  |  4 |  5 |  6 |  7 |\n",
    "  ---------------------\n",
    "  |  8 |  9 | 10 | 11 |\n",
    "  ---------------------\n",
    "  | 12 | 13 | 14 | 15 |\n",
    "  ---------------------\n",
    "\n",
    "  Goal state: 11 \n",
    "  Bad state: 9\n",
    "  End state: 16\n",
    "\n",
    "  The end state is an absorbing state that the agent transitions \n",
    "  to after visiting the goal state.\n",
    "\n",
    "  There are 17 states in total (including the end state) \n",
    "  and 4 actions (up, down, left, right).'''\n",
    "\n",
    "# Transition function: |A| x |S| x |S'| array\n",
    "T = np.zeros([4,17,17])\n",
    "a = 0.8;  # intended move\n",
    "b = 0.1;  # lateral move\n",
    "\n",
    "# up (a = 0)\n",
    "\n",
    "T[0,0,0] = a+b;\n",
    "T[0,0,1] = b;\n",
    "\n",
    "T[0,1,0] = b;\n",
    "T[0,1,1] = a;\n",
    "T[0,1,2] = b;\n",
    "\n",
    "T[0,2,1] = b;\n",
    "T[0,2,2] = a;\n",
    "T[0,2,3] = b;\n",
    "\n",
    "T[0,3,2] = b;\n",
    "T[0,3,3] = a+b;\n",
    "\n",
    "T[0,4,4] = b;\n",
    "T[0,4,0] = a;\n",
    "T[0,4,5] = b;\n",
    "\n",
    "T[0,5,4] = b;\n",
    "T[0,5,1] = a;\n",
    "T[0,5,6] = b;\n",
    "\n",
    "T[0,6,5] = b;\n",
    "T[0,6,2] = a;\n",
    "T[0,6,7] = b;\n",
    "\n",
    "T[0,7,6] = b;\n",
    "T[0,7,3] = a;\n",
    "T[0,7,7] = b;\n",
    "\n",
    "T[0,8,8] = b;\n",
    "T[0,8,4] = a;\n",
    "T[0,8,9] = b;\n",
    "\n",
    "T[0,9,8] = b;\n",
    "T[0,9,5] = a;\n",
    "T[0,9,10] = b;\n",
    "\n",
    "T[0,10,9] = b;\n",
    "T[0,10,6] = a;\n",
    "T[0,10,11] = b;\n",
    "\n",
    "T[0,11,16] = 1;\n",
    "\n",
    "T[0,12,12] = b;\n",
    "T[0,12,8] = a;\n",
    "T[0,12,13] = b;\n",
    "\n",
    "T[0,13,12] = b;\n",
    "T[0,13,9] = a;\n",
    "T[0,13,14] = b;\n",
    "\n",
    "T[0,14,13] = b;\n",
    "T[0,14,10] = a;\n",
    "T[0,14,15] = b;\n",
    "\n",
    "T[0,15,14] = b;\n",
    "T[0,15,11] = a;\n",
    "T[0,15,15] = b;\n",
    "\n",
    "T[0,16,16] = 1;\n",
    "\n",
    "# down (a = 1)\n",
    "\n",
    "T[1,0,0] = b;\n",
    "T[1,0,4] = a;\n",
    "T[1,0,1] = b;\n",
    "\n",
    "T[1,1,0] = b;\n",
    "T[1,1,5] = a;\n",
    "T[1,1,2] = b;\n",
    "\n",
    "T[1,2,1] = b;\n",
    "T[1,2,6] = a;\n",
    "T[1,2,3] = b;\n",
    "\n",
    "T[1,3,2] = b;\n",
    "T[1,3,7] = a;\n",
    "T[1,3,3] = b;\n",
    "\n",
    "T[1,4,4] = b;\n",
    "T[1,4,8] = a;\n",
    "T[1,4,5] = b;\n",
    "\n",
    "T[1,5,4] = b;\n",
    "T[1,5,9] = a;\n",
    "T[1,5,6] = b;\n",
    "\n",
    "T[1,6,5] = b;\n",
    "T[1,6,10] = a;\n",
    "T[1,6,7] = b;\n",
    "\n",
    "T[1,7,6] = b;\n",
    "T[1,7,11] = a;\n",
    "T[1,7,7] = b;\n",
    "\n",
    "T[1,8,8] = b;\n",
    "T[1,8,12] = a;\n",
    "T[1,8,9] = b;\n",
    "\n",
    "T[1,9,8] = b;\n",
    "T[1,9,13] = a;\n",
    "T[1,9,10] = b;\n",
    "\n",
    "T[1,10,9] = b;\n",
    "T[1,10,14] = a;\n",
    "T[1,10,11] = b;\n",
    "\n",
    "T[1,11,16] = 1;\n",
    "\n",
    "T[1,12,12] = a+b;\n",
    "T[1,12,13] = b;\n",
    "\n",
    "T[1,13,12] = b;\n",
    "T[1,13,13] = a;\n",
    "T[1,13,14] = b;\n",
    "\n",
    "T[1,14,13] = b;\n",
    "T[1,14,14] = a;\n",
    "T[1,14,15] = b;\n",
    "\n",
    "T[1,15,14] = b;\n",
    "T[1,15,15] = a+b;\n",
    "\n",
    "T[1,16,16] = 1;\n",
    "\n",
    "# left (a = 2)\n",
    "\n",
    "T[2,0,0] = a+b;\n",
    "T[2,0,4] = b;\n",
    "\n",
    "T[2,1,1] = b;\n",
    "T[2,1,0] = a;\n",
    "T[2,1,5] = b;\n",
    "\n",
    "T[2,2,2] = b;\n",
    "T[2,2,1] = a;\n",
    "T[2,2,6] = b;\n",
    "\n",
    "T[2,3,3] = b;\n",
    "T[2,3,2] = a;\n",
    "T[2,3,7] = b;\n",
    "\n",
    "T[2,4,0] = b;\n",
    "T[2,4,4] = a;\n",
    "T[2,4,8] = b;\n",
    "\n",
    "T[2,5,1] = b;\n",
    "T[2,5,4] = a;\n",
    "T[2,5,9] = b;\n",
    "\n",
    "T[2,6,2] = b;\n",
    "T[2,6,5] = a;\n",
    "T[2,6,10] = b;\n",
    "\n",
    "T[2,7,3] = b;\n",
    "T[2,7,6] = a;\n",
    "T[2,7,11] = b;\n",
    "\n",
    "T[2,8,4] = b;\n",
    "T[2,8,8] = a;\n",
    "T[2,8,12] = b;\n",
    "\n",
    "T[2,9,5] = b;\n",
    "T[2,9,8] = a;\n",
    "T[2,9,13] = b;\n",
    "\n",
    "T[2,10,6] = b;\n",
    "T[2,10,9] = a;\n",
    "T[2,10,14] = b;\n",
    "\n",
    "T[2,11,16] = 1;\n",
    "\n",
    "T[2,12,8] = b;\n",
    "T[2,12,12] = a+b;\n",
    "\n",
    "T[2,13,9] = b;\n",
    "T[2,13,12] = a;\n",
    "T[2,13,13] = b;\n",
    "\n",
    "T[2,14,10] = b;\n",
    "T[2,14,13] = a;\n",
    "T[2,14,14] = b;\n",
    "\n",
    "T[2,15,11] = b;\n",
    "T[2,15,14] = a;\n",
    "T[2,15,15] = b;\n",
    "\n",
    "T[2,16,16] = 1;\n",
    "\n",
    "# right (a = 3)\n",
    "\n",
    "T[3,0,0] = b;\n",
    "T[3,0,1] = a;\n",
    "T[3,0,4] = b;\n",
    "\n",
    "T[3,1,1] = b;\n",
    "T[3,1,2] = a;\n",
    "T[3,1,5] = b;\n",
    "\n",
    "T[3,2,2] = b;\n",
    "T[3,2,3] = a;\n",
    "T[3,2,6] = b;\n",
    "\n",
    "T[3,3,3] = a+b;\n",
    "T[3,3,7] = b;\n",
    "\n",
    "T[3,4,0] = b;\n",
    "T[3,4,5] = a;\n",
    "T[3,4,8] = b;\n",
    "\n",
    "T[3,5,1] = b;\n",
    "T[3,5,6] = a;\n",
    "T[3,5,9] = b;\n",
    "\n",
    "T[3,6,2] = b;\n",
    "T[3,6,7] = a;\n",
    "T[3,6,10] = b;\n",
    "\n",
    "T[3,7,3] = b;\n",
    "T[3,7,7] = a;\n",
    "T[3,7,11] = b;\n",
    "\n",
    "T[3,8,4] = b;\n",
    "T[3,8,9] = a;\n",
    "T[3,8,12] = b;\n",
    "\n",
    "T[3,9,5] = b;\n",
    "T[3,9,10] = a;\n",
    "T[3,9,13] = b;\n",
    "\n",
    "T[3,10,6] = b;\n",
    "T[3,10,11] = a;\n",
    "T[3,10,14] = b;\n",
    "\n",
    "T[3,11,16] = 1;\n",
    "\n",
    "T[3,12,8] = b;\n",
    "T[3,12,13] = a;\n",
    "T[3,12,12] = b;\n",
    "\n",
    "T[3,13,9] = b;\n",
    "T[3,13,14] = a;\n",
    "T[3,13,13] = b;\n",
    "\n",
    "T[3,14,10] = b;\n",
    "T[3,14,15] = a;\n",
    "T[3,14,14] = b;\n",
    "\n",
    "T[3,15,11] = b;\n",
    "T[3,15,15] = a+b;\n",
    "\n",
    "T[3,16,16] = 1;\n",
    "\n",
    "# Reward function: |A| x |S| array\n",
    "R = -1 * np.ones([4,17]);\n",
    "\n",
    "# set rewards\n",
    "R[:,11] = 100;  # goal state\n",
    "R[:,9] = -70;   # bad state\n",
    "R[:,16] = 0;    # end state\n",
    "\n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MDP object\n",
    "mdp =MDP(T,R,discount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Each Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration...\n",
      "[V,iterId,epsilon] = [array([ 66.4750799 ,  72.35601113,  78.52647962,  84.21513533,\n",
      "        64.96977905,  71.61263898,  84.87838464,  91.78276307,\n",
      "        55.06163193,  12.96639802,  91.19633961, 100.        ,\n",
      "        65.26377228,  72.14936837,  85.60984747,  91.85958892,\n",
      "         0.        ]), 18, 0.005916217653677336]\n"
     ]
    }
   ],
   "source": [
    "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates),\n",
    "                                             tolerance=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "[policy,V,nIterations] = mdp.policyIteration(np.zeros(mdp.nStates,dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 0, 3, 3, 3, 0, 0]),\n",
       " array([ 66.48026629,  72.35814698,  78.52739639,  84.21554544,\n",
       "         64.97315039,  71.6137027 ,  84.87872847,  91.78284995,\n",
       "         55.06432671,  12.96679046,  91.19641953, 100.        ,\n",
       "         65.26496129,  72.14957754,  85.60989813,  91.85960257,\n",
       "          0.        ]),\n",
       " 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, V, nIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 0, 3, 3, 3, 0, 0]),\n",
       " array([ 66.47162218,  72.35454333,  78.52585639,  84.21485392,\n",
       "         64.9677271 ,  71.61193002,  84.87814924,  91.78270463,\n",
       "         55.06093332,  12.96615491,  91.19629119, 100.        ,\n",
       "         65.26344413,  72.14929159,  85.60983295,  91.85958651,\n",
       "          0.        ]),\n",
       " 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[policy,V,nIterations,epsilon] = mdp.modifiedPolicyIteration(np.zeros(mdp.nStates,dtype=int),np.zeros(mdp.nStates),tolerance=0.01)\n",
    "policy, V, nIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
